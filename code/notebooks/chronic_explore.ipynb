{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyPerceive to select and load Percept recordings "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0a. Loading default packages and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Python and external packages\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import json\n",
    "from dataclasses import dataclass, field, fields\n",
    "from itertools import compress\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import signal\n",
    "\n",
    "import openpyxl\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import xlrd\n",
    "\n",
    "#mne\n",
    "import mne_bids\n",
    "import mne\n",
    "from mne.time_frequency import tfr_morlet \n",
    "\n",
    "from importlib import reload          \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### check package versions\n",
    "\n",
    "developed with:\n",
    "- Python sys 3.10.8\n",
    "- pandas 1.5.1\n",
    "- numpy 1.23.4\n",
    "- mne_bids 0.11.1\n",
    "- mne 1.2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check some package versions for documentation and reproducability\n",
    "print('Python sys', sys.version)\n",
    "print('pandas', pd.__version__)\n",
    "print('numpy', np.__version__)\n",
    "print('mne_bids', mne_bids.__version__)\n",
    "print('mne', mne.__version__)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0b. Loading pyPerceive functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_and_set_code_folder_in_notebook():\n",
    "    \"\"\"\n",
    "    while working in the local pyPerceive repo,\n",
    "    find and set path to the PyPerceive code folder\n",
    "\n",
    "    use function in notebook first, to locate the local\n",
    "    repo and enable import of pyPerceive functions\n",
    "    \"\"\"\n",
    "    project_path = os.getcwd()\n",
    "\n",
    "    while project_path[-10:] != 'PyPerceive':\n",
    "        project_path = os.path.dirname(project_path)\n",
    "\n",
    "    code_path = os.path.join(project_path, 'code')\n",
    "    sys.path.append(code_path)\n",
    "\n",
    "    # change directory to code path\n",
    "    os.chdir(code_path)\n",
    "    \n",
    "    return print(f'working dir set to: {code_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MAIN FUNCTION FOR DATA IMPORT\n",
    "\n",
    "# change working directory to ensure correct loading of own functions\n",
    "add_and_set_code_folder_in_notebook()\n",
    "\n",
    "# import main class to work with\n",
    "from PerceiveImport.classes import main_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT ALL SUB CLASSES AND FUNCTIONS FOR DEBUGGING\n",
    "from PerceiveImport.classes import (\n",
    "    main_class, modality_class, metadata_class,\n",
    "    session_class, condition_class, task_class,\n",
    "    contact_class, run_class, chronic_class\n",
    ")\n",
    "import PerceiveImport.methods.load_rawfile as load_rawfile\n",
    "import PerceiveImport.methods.find_folders as find_folders\n",
    "import PerceiveImport.methods.metadata_helpers as metaHelpers\n",
    "import PerceiveImport.methods.timezone_handling as tz_handling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load Streaming, Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an example instance and fill in the values of the dataclass PerceiveData \n",
    "# choose the values you are interested in analyzing further\n",
    "\n",
    "sub024 = main_class.PerceiveData(\n",
    "    sub = \"024\", \n",
    "    incl_modalities=['survey', 'streaming', 'indefiniteStreaming'],\n",
    "    incl_session = [\"fu18m\"],\n",
    "    incl_condition =['m0s0'],\n",
    "    incl_task = [\"rest\"],\n",
    "    incl_contact = [\"RingL\", \"SegmInterR\", \"SegmIntraR\"],\n",
    "    import_json=False,\n",
    "    warn_for_metaNaNs=True,\n",
    "    # use_bids=True,  # TODO: add to functionality\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub024.streaming.fu18m.m0s0.rest.run1.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub024.survey.fu18m.m0s0.rest.SegmInterR.run1.data.get_data()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading Chronic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of debugging\n",
    "from PerceiveImport.methods import extract_chronic_timeline_samples as extract_chronic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chronic_subs = ['039', '040', '041', '059', '070', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an example instance and fill in the values of the dataclass PerceiveData \n",
    "# choose the values you are interested in analyzing further\n",
    "\n",
    "importlib.reload(tz_handling)\n",
    "importlib.reload(extract_chronic)\n",
    "\n",
    "importlib.reload(chronic_class)\n",
    "\n",
    "dat = main_class.PerceiveData(\n",
    "    sub = chronic_subs[-2], \n",
    "    incl_modalities=['chronic'],\n",
    "    import_json=True,\n",
    "    warn_for_metaNaNs=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dat.chronic.data['local_time'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = np.argsort(dat.chronic.data['local_time'].values[:300])#[::-1]\n",
    "\n",
    "dat.chronic.data.iloc[sort_idx, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dat.chronic.data['local_time'],\n",
    "         dat.chronic.data['PSD_Left'])\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_l = dat.chronic.data['PSD_Left'].values\n",
    "dat_r = dat.chronic.data['PSD_Right'].values\n",
    "f_l = float(dat.chronic.data['freq_Left'][0])\n",
    "f_r = float(dat.chronic.data['freq_Right'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "i2 = 1500\n",
    "\n",
    "times = dat.chronic.data['local_time'].values[i2:]\n",
    "\n",
    "ax.plot(times, dat_l[i2:], label=f'left ({f_l} Hz)')\n",
    "ax.plot(times, dat_r[i2:], label=f'right ({f_r} Hz)')\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "ax.set_ylim(0, 1500)\n",
    "\n",
    "# xticks = np.linspace(0, 1500, 10).astype(int)\n",
    "# ax.set_xticks(xticks)\n",
    "# ax.set_xticklabels(np.array(times)[xticks])\n",
    "ax.tick_params(axis='x', rotation=90)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Direct access JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_fname = 'Report_Json_Session_Report_20230523T113952.json'\n",
    "j = load_rawfile.load_sourceJSON('047', json_fname)\n",
    "\n",
    "json_fname = 'Report_Json_Session_Report_20230524T150050.json'\n",
    "j2 = load_rawfile.load_sourceJSON('047', json_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_list_string_floats(\n",
    "    string_list\n",
    "):\n",
    "    try:\n",
    "        floats = [float(v) for v in string_list.split(',')]\n",
    "    except:\n",
    "        floats = [float(v) for v in string_list[:-1].split(',')]\n",
    "\n",
    "    return floats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(j.keys())\n",
    "print(j2.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prc_data_codes = {\n",
    "    'signal_test': 'CalibrationTests',\n",
    "    'streaming': 'BrainSenseTimeDomain',\n",
    "    'survey': 'LfpMontageTimeDomain',\n",
    "    'indef_streaming': 'IndefiniteStreaming'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j.keys()\n",
    "\n",
    "j['BrainSenseLfp'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# j[prc_data_codes['survey']]  # TO INTEGRATE\n",
    "\n",
    "check_and_correct_missings_in_lfp(j['BrainSenseLfp'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = 'streaming'\n",
    "\n",
    "list_of_streamings = j[prc_data_codes[mod]]\n",
    "n_streamings = len(list_of_streamings)\n",
    "\n",
    "\n",
    "list_of_streamings2 = j2[prc_data_codes[mod]]\n",
    "\n",
    "# n_exp_streamings = extract from metadata\n",
    "# check whether n-streamings match metdata table \n",
    "# if n_streamings == n_streamings: ...\n",
    "\n",
    "for dat in list_of_streamings2:\n",
    "    print(dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_streamings[0]['GlobalPacketSizes']\n",
    "\n",
    "print(list_of_streamings[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dat = list_of_streamings[0]\n",
    "for i_dat, dat in enumerate(list_of_streamings):\n",
    "    print(i_dat)\n",
    "    new_lfp = check_and_correct_missings_in_lfp(dat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_correct_missings_in_lfp(dat: dict):\n",
    "    \"\"\"\"\n",
    "    Function checks missing packets based on start and endtime\n",
    "    of first and last received packets, and the time-differences\n",
    "    between consecutive packets. In case of a missing packet,\n",
    "    the missing time window is filled with NaNs.\n",
    "\n",
    "    WORKS NOW FOR BRAINSENSETIMEDOMAIN DATA STRUCTURE, NOT YET\n",
    "    FOR BRAINSENSELFP OR SURVEY, STREAMING\n",
    "    \"\"\"\n",
    "    Fs = dat['SampleRateInHz']\n",
    "    ticksMsec = convert_list_string_floats(dat['TicksInMses'])\n",
    "    ticksDiffs = np.diff(np.array(ticksMsec))\n",
    "    data_is_missing = (ticksDiffs != 250).any()\n",
    "    packetSizes = convert_list_string_floats(dat['GlobalPacketSizes'])\n",
    "    lfp_data = dat['TimeDomainData']\n",
    "\n",
    "    if data_is_missing:\n",
    "        print('LFP Data is missing!! perform function to fill NaNs in')\n",
    "    else:\n",
    "        print('No LFP data missing based on timestamp '\n",
    "            'differences between data-packets')\n",
    "\n",
    "    data_length_ms = ticksMsec[-1] + 250 - ticksMsec[0]  # length of a pakcet in milliseconds is always 250\n",
    "    data_length_samples = int(data_length_ms / 1000 * Fs) + 1  # add one to calculate for 63 packet at end\n",
    "    new_lfp_arr = np.array([np.nan] * data_length_samples)\n",
    "\n",
    "    # fill nan array with real LFP values, use tickDiffs to decide start-points (and where to leave NaN)\n",
    "\n",
    "    # Add first packet (data always starts with present packet)\n",
    "    current_packetSize = int(packetSizes[0])\n",
    "    if current_packetSize > 63:\n",
    "        print(f'UNKNOWN TOO LARGE DATAPACKET IS CUTDOWN BY {current_packetSize - 63} samples')\n",
    "        current_packetSize = 63  # if there is UNKNOWN TOO MANY DATA, only the first 63 samples of the too large packets are included\n",
    "\n",
    "    new_lfp_arr[:current_packetSize] = lfp_data[:current_packetSize]\n",
    "    # loop over every distance (index for packetsize is + 1 because first difference corresponds to seconds packet)\n",
    "    i_lfp = current_packetSize  # index to track which lfp values are already used\n",
    "    i_arr = current_packetSize  # index to track of new array index\n",
    "    \n",
    "    i_packet = 1\n",
    "\n",
    "    for diff in ticksDiffs:\n",
    "        if diff == 250:\n",
    "            # only lfp values, no nans if distance was 250 ms\n",
    "            current_packetSize = int(packetSizes[i_packet])\n",
    "\n",
    "            # in case of very rare TOO LARGE packetsize (there is MORE DATA than expected based on the first and last timestamps)\n",
    "            if current_packetSize > 63:\n",
    "                print(f'UNKNOWN TOO LARGE DATAPACKET IS CUTDOWN BY {current_packetSize - 63} samples')\n",
    "                current_packetSize = 63\n",
    "\n",
    "            new_lfp_arr[\n",
    "                i_arr:int(i_arr + current_packetSize)\n",
    "            ] = lfp_data[i_lfp:int(i_lfp + current_packetSize)]\n",
    "            i_lfp += current_packetSize\n",
    "            i_arr += current_packetSize\n",
    "            i_packet += 1\n",
    "        else:\n",
    "            print('add NaNs by skipping')\n",
    "            msecs_missing = (diff - 250)  # difference if one packet is missing is 500 ms\n",
    "            \n",
    "            secs_missing = msecs_missing / 1000\n",
    "            samples_missing = int(secs_missing * Fs)\n",
    "            # no filling with NaNs, bcs array is created full with NaNs\n",
    "            i_arr += samples_missing  # shift array index up by number of NaNs left in the array\n",
    "    \n",
    "    # correct in case one sample too many was in array shape\n",
    "    if np.isnan(new_lfp_arr[-1]): new_lfp_arr = new_lfp_arr[:-1]\n",
    "\n",
    "    return new_lfp_arr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyPerceive",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "89cb9b15ea7fbcc6bc9b1c7e86ec8f92184be73d513127a97a923adf23b86793"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
